{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_highlevel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omb8325Q3FpK",
        "colab_type": "text"
      },
      "source": [
        "# More Realistic Language Modeling & Recurrent Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjnw40GvuXVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8zBF_hJut4i",
        "colab_type": "code",
        "outputId": "d4edb705-ebdb-449a-e3ac-81c0980936ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/f9/0626bbdb322e3a078d968e87e3b01341e7890544de891d0cb613641220e6/ipython-autotime-0.1.tar.bz2\n",
            "Building wheels for collected packages: ipython-autotime\n",
            "  Building wheel for ipython-autotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipython-autotime: filename=ipython_autotime-0.1-cp36-none-any.whl size=1832 sha256=2b9ec372600ddf66d82fd79983fce1145c7ee5ccd1f49b8a2b7e8ca7c8a750c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/df/81/2db1e54bc91002cec40334629bc39cfa86dff540b304ebcd6e\n",
            "Successfully built ipython-autotime\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTX1iVIXvoRy",
        "colab_type": "code",
        "outputId": "ea0d026c-0466-4d76-e30e-844cebf6815c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie1E2UtXvqqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCldKJZmv1zZ",
        "colab_type": "code",
        "outputId": "e4e3ec24-ad9a-4fc5-ddd4-10b5fcb383a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6R4zfmpuvk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T41zbmW44HB",
        "colab_type": "code",
        "outputId": "09287af3-8eff-424d-bdbb-b85872577755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.keras.utils.get_file(\"/content/drive/My Drive/Colab Notebooks/shakespeare.txt\", \n",
        "                        \"https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/shakespeare.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "text": [
            "time: 2.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7106vXP_pP-",
        "colab_type": "text"
      },
      "source": [
        "# Re-implementation of low-level RNN with Keras functionality\n",
        "\n",
        "Reference: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8Dt0SRi3JvO",
        "colab_type": "code",
        "outputId": "8729f0f8-14a0-411b-d247-ce687e9c5d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python prepare_data.py shakespeare.txt skp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-01 12:31:13.725661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 22981 sequences...\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n",
            "time: 5.35 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCappyAz6hGP",
        "colab_type": "code",
        "outputId": "19a14c05-73cb-41a3-c8ea-81518c78fc58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from prepare_data import parse_seq\n",
        "import pickle\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "seq_len = 200\n",
        "data = data.map(lambda x: parse_seq(x, seq_len))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\")) \n",
        "#unpickling-convert byte stream to python object structure #read bytes\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab_size) #26(uppercase)+26(lowercase)+16(special characters)=68\n",
        "print(vocab) \n",
        "print(ind_to_ch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68\n",
            "{'n': 1, 'R': 2, ':': 3, \"'\": 4, 'B': 5, 'g': 6, 'M': 7, 'P': 8, 'w': 9, '3': 10, 'a': 11, 'L': 12, '\\n': 13, 'A': 14, 'b': 15, 'X': 16, '.': 17, '&': 18, 'Z': 19, '!': 20, 'f': 21, 'i': 22, 'u': 23, 'q': 24, 'K': 25, 'm': 26, 'N': 27, 's': 28, '-': 29, 'r': 30, 'E': 31, 'z': 32, 'p': 33, 'l': 34, '$': 35, 'c': 36, 'S': 37, 'H': 38, 'd': 39, 'y': 40, 'J': 41, 'C': 42, 'o': 43, 'h': 44, 't': 45, '?': 46, 'T': 47, 'Q': 48, ';': 49, 'W': 50, 'O': 51, 'Y': 52, 'v': 53, 'I': 54, 'U': 55, 'D': 56, '[': 57, 'G': 58, ']': 59, 'e': 60, 'x': 61, ',': 62, 'k': 63, 'F': 64, 'j': 65, ' ': 66, 'V': 67, '<S>': 0}\n",
            "{1: 'n', 2: 'R', 3: ':', 4: \"'\", 5: 'B', 6: 'g', 7: 'M', 8: 'P', 9: 'w', 10: '3', 11: 'a', 12: 'L', 13: '\\n', 14: 'A', 15: 'b', 16: 'X', 17: '.', 18: '&', 19: 'Z', 20: '!', 21: 'f', 22: 'i', 23: 'u', 24: 'q', 25: 'K', 26: 'm', 27: 'N', 28: 's', 29: '-', 30: 'r', 31: 'E', 32: 'z', 33: 'p', 34: 'l', 35: '$', 36: 'c', 37: 'S', 38: 'H', 39: 'd', 40: 'y', 41: 'J', 42: 'C', 43: 'o', 44: 'h', 45: 't', 46: '?', 47: 'T', 48: 'Q', 49: ';', 50: 'W', 51: 'O', 52: 'Y', 53: 'v', 54: 'I', 55: 'U', 56: 'D', 57: '[', 58: 'G', 59: ']', 60: 'e', 61: 'x', 62: ',', 63: 'k', 64: 'F', 65: 'j', 66: ' ', 67: 'V', 0: '<S>'}\n",
            "time: 796 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMNoyK9OxGjb",
        "colab_type": "code",
        "outputId": "9fe4761a-9c3c-4823-ad48-ef9e1022e8a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = data.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 51.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmzuN-Iwx-oG",
        "colab_type": "code",
        "outputId": "f5cf15f3-c4d7-4ad4-8c4b-0345875cbaee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 199), (64, 199)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "stream",
          "text": [
            "time: 6.11 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6-gm61t1T75",
        "colab_type": "code",
        "outputId": "f07e33d3-f6e8-4e27-945c-6422d7ae3a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.35 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMirhvHu11Ii",
        "colab_type": "code",
        "outputId": "e9972d11-8266-43c0-99c3-de93f7bba7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.46 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G4r4Ns715Tj",
        "colab_type": "code",
        "outputId": "857b051d-73bb-4985-be34-949bdcfda67c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 219 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABcvyxdn19Ty",
        "colab_type": "code",
        "outputId": "276903d4-51a9-43f6-f458-d6b21e05ff21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           17408     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 68)            69700     \n",
            "=================================================================\n",
            "Total params: 4,025,412\n",
            "Trainable params: 4,025,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 3.21 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6BxzpAZ2i1f",
        "colab_type": "code",
        "outputId": "3cf6ef24-8d2c-47a0-9fc5-8345b5b0358d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 199, 68) # (batch_size, sequence_length, vocab_size)\n",
            "time: 6.87 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceZETepq1_tr",
        "colab_type": "code",
        "outputId": "8b44e876-84fc-4936-8fff-79544dab22b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 199, 68)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.219768\n",
            "time: 13.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUImJ8Rd2ES0",
        "colab_type": "code",
        "outputId": "2768340c-9001-4b35-dc9e-adf8323dae9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 16.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVD70A2w2GK2",
        "colab_type": "code",
        "outputId": "86299bc2-f3eb-469b-a5f9-8bf207f5e2db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.66 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GUJQ6Nf2H8f",
        "colab_type": "code",
        "outputId": "7c691c26-f6b5-47e0-fd28-0363b18582dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 895 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDh51adm_0L0",
        "colab_type": "text"
      },
      "source": [
        "# Training- Using model.fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjhtvNCP2Jj9",
        "colab_type": "code",
        "outputId": "f778dd4f-d095-4100-f653-294e9cd22f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "359/359 [==============================] - 40s 111ms/step - loss: 2.3024\n",
            "Epoch 2/10\n",
            "359/359 [==============================] - 39s 108ms/step - loss: 1.5971\n",
            "Epoch 3/10\n",
            "359/359 [==============================] - 39s 109ms/step - loss: 1.4116\n",
            "Epoch 4/10\n",
            "359/359 [==============================] - 39s 108ms/step - loss: 1.3324\n",
            "Epoch 5/10\n",
            "359/359 [==============================] - 39s 109ms/step - loss: 1.2853\n",
            "Epoch 6/10\n",
            "359/359 [==============================] - 39s 109ms/step - loss: 1.2505\n",
            "Epoch 7/10\n",
            "359/359 [==============================] - 39s 109ms/step - loss: 1.2226\n",
            "Epoch 8/10\n",
            "359/359 [==============================] - 39s 109ms/step - loss: 1.1980\n",
            "Epoch 9/10\n",
            "359/359 [==============================] - 39s 109ms/step - loss: 1.1762\n",
            "Epoch 10/10\n",
            "359/359 [==============================] - 39s 109ms/step - loss: 1.1559\n",
            "time: 6min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5xJGL3q3xeW",
        "colab_type": "text"
      },
      "source": [
        "# Text generation- Keeping batch size = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D2t6YFX5M-j",
        "colab_type": "code",
        "outputId": "c3b6abaa-af70-41ca-8aa8-0f4ac055b3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "stream",
          "text": [
            "time: 9.66 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG6F1qDt5Q-S",
        "colab_type": "code",
        "outputId": "8269a846-2b5f-4c76-b2ef-1db5fb3d505a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 288 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEFdBPaO5Tn8",
        "colab_type": "code",
        "outputId": "dff93df1-8877-405f-e759-c9364518e917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            17408     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 68)             69700     \n",
            "=================================================================\n",
            "Total params: 4,025,412\n",
            "Trainable params: 4,025,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 2.42 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7DuAGKs5YgG",
        "colab_type": "code",
        "outputId": "6cb8adec-84e4-4c88-aa94-e5c820c71b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [vocab[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(ind_to_ch[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 16.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuuDKSAp5eVo",
        "colab_type": "code",
        "outputId": "a1e3b0bd-21c9-4143-bf4b-3688d5df959b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: O, if thou dies!\n",
            "\n",
            "JULIA:\n",
            "O, let me speak; speak for you:\n",
            "We do not know it; I have known your praise.\n",
            "How many father?\n",
            "\n",
            "MISTRESS QUICKLY:\n",
            "I'll prove it.\n",
            "\n",
            "IMOGEN:\n",
            "Some strange instruments, good sir,\n",
            "Are the same DouR ROSH:\n",
            "What ring!\n",
            "\n",
            "CASCI:\n",
            "Fare you well.\n",
            "\n",
            "First Senator:\n",
            "Now shall it be, when I made daintin than\n",
            "forger: if nothing was fot once our vow'd before,\n",
            "We swallowed by the specularly break?\n",
            "Now king is him: how show'd me solely gold,\n",
            "Shall poor a worthy as confine before!\n",
            "\n",
            "VALENTINE:\n",
            "This night it speedless: I will murder me,\n",
            "Gloucester.\n",
            "\n",
            "HASTINGS:\n",
            "My lord, bravely have your services\n",
            "Of all my dowry, now I do not know--as I bide\n",
            "The king so much disordeth as a body\n",
            "That our general expeditions.\n",
            "\n",
            "Gien:\n",
            "Four-if would do hair of holy commonwealth!\n",
            "\n",
            "LUCENTIO:\n",
            "You shall have long even in that house I find to\n",
            "do't: come, my cow whence recounters: their passions\n",
            "From forpease whose hoppiness to wear our\n",
            "tents to antent upon old and a dullew;\n",
            "Who, I am king in sword again, or then;\n",
            "Ver\n",
            "time: 4.41 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO4-5iPN7Fao",
        "colab_type": "text"
      },
      "source": [
        "# With custom training loop\n",
        "using tf.GradientTape()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Y6oKHZ7B5j",
        "colab_type": "code",
        "outputId": "75d4aec5-1051-40de-9939-ffd065be8224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 210 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "END7Y2TK7M2B",
        "colab_type": "code",
        "outputId": "a30d9f30-152d-4773-8896-197c00341f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.04 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpaYmp-_7OZY",
        "colab_type": "code",
        "outputId": "c770b42c-acc3-4a2d-9e36-3b32cb1d1add",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.04 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_QT8FDA7Qks",
        "colab_type": "code",
        "outputId": "8d6dfc21-14d3-47c9-cde9-45fc738b7d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.21998929977417\n",
            "Epoch 1 Batch 100 Loss 2.3428797721862793\n",
            "Epoch 1 Batch 200 Loss 2.0970232486724854\n",
            "Epoch 1 Batch 300 Loss 1.8865776062011719\n",
            "Epoch 1 Loss 1.7760\n",
            "Time taken for 1 epoch 40.94127154350281 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.7895363569259644\n",
            "Epoch 2 Batch 100 Loss 1.6617414951324463\n",
            "Epoch 2 Batch 200 Loss 1.5498625040054321\n",
            "Epoch 2 Batch 300 Loss 1.494864821434021\n",
            "Epoch 2 Loss 1.5081\n",
            "Time taken for 1 epoch 38.597591400146484 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.4671552181243896\n",
            "Epoch 3 Batch 100 Loss 1.4405620098114014\n",
            "Epoch 3 Batch 200 Loss 1.4301587343215942\n",
            "Epoch 3 Batch 300 Loss 1.390113115310669\n",
            "Epoch 3 Loss 1.3489\n",
            "Time taken for 1 epoch 39.1321005821228 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.3332915306091309\n",
            "Epoch 4 Batch 100 Loss 1.333824872970581\n",
            "Epoch 4 Batch 200 Loss 1.3624721765518188\n",
            "Epoch 4 Batch 300 Loss 1.347478985786438\n",
            "Epoch 4 Loss 1.3610\n",
            "Time taken for 1 epoch 38.81589198112488 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.3021132946014404\n",
            "Epoch 5 Batch 100 Loss 1.3141967058181763\n",
            "Epoch 5 Batch 200 Loss 1.2704967260360718\n",
            "Epoch 5 Batch 300 Loss 1.2901376485824585\n",
            "Epoch 5 Loss 1.2840\n",
            "Time taken for 1 epoch 39.07834839820862 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.2228803634643555\n",
            "Epoch 6 Batch 100 Loss 1.2644065618515015\n",
            "Epoch 6 Batch 200 Loss 1.2726354598999023\n",
            "Epoch 6 Batch 300 Loss 1.2480440139770508\n",
            "Epoch 6 Loss 1.2590\n",
            "Time taken for 1 epoch 38.99924993515015 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2453713417053223\n",
            "Epoch 7 Batch 100 Loss 1.2435349225997925\n",
            "Epoch 7 Batch 200 Loss 1.216905951499939\n",
            "Epoch 7 Batch 300 Loss 1.2474963665008545\n",
            "Epoch 7 Loss 1.2789\n",
            "Time taken for 1 epoch 38.90287518501282 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.182608723640442\n",
            "Epoch 8 Batch 100 Loss 1.1870182752609253\n",
            "Epoch 8 Batch 200 Loss 1.234596610069275\n",
            "Epoch 8 Batch 300 Loss 1.1822636127471924\n",
            "Epoch 8 Loss 1.1869\n",
            "Time taken for 1 epoch 38.89557194709778 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.1753898859024048\n",
            "Epoch 9 Batch 100 Loss 1.1491427421569824\n",
            "Epoch 9 Batch 200 Loss 1.1976768970489502\n",
            "Epoch 9 Batch 300 Loss 1.1534790992736816\n",
            "Epoch 9 Loss 1.2279\n",
            "Time taken for 1 epoch 38.915528535842896 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.138946294784546\n",
            "Epoch 10 Batch 100 Loss 1.1183675527572632\n",
            "Epoch 10 Batch 200 Loss 1.188766598701477\n",
            "Epoch 10 Batch 300 Loss 1.1724809408187866\n",
            "Epoch 10 Loss 1.1708\n",
            "Time taken for 1 epoch 38.93033003807068 sec\n",
            "\n",
            "time: 6min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7htQs-8q_6X9",
        "colab_type": "text"
      },
      "source": [
        "# Dealing with Variable-length Sequences\n",
        "\n",
        "## With padding and masking\n",
        "\n",
        "Reference: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh8me0EVG6pH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8oOd7XLHKJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzA2xIVh4MiG",
        "colab_type": "text"
      },
      "source": [
        "Longest remaining sequence has length 499."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xncWkmOa_5-r",
        "colab_type": "code",
        "outputId": "5fc3378e-d977-4ca5-9669-5a1dd3ff86da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python prepare_data2.py  shakespeare.txt shake \\\\n\\\\n+ --maxlen 500"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-02 07:27:41.224032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 31022 sequences...\n",
            "Longest sequence is 3094 characters. If this seems unreasonable, consider using the maxlen argument!\n",
            "Removing sequences longer than 500 characters...\n",
            "29429 sequences remaining.\n",
            "Longest remaining sequence has length 499.\n",
            "Removing length-0 sequences...\n",
            "29429 sequences remaining.\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n",
            "Serialized 23000 sequences...\n",
            "Serialized 23100 sequences...\n",
            "Serialized 23200 sequences...\n",
            "Serialized 23300 sequences...\n",
            "Serialized 23400 sequences...\n",
            "Serialized 23500 sequences...\n",
            "Serialized 23600 sequences...\n",
            "Serialized 23700 sequences...\n",
            "Serialized 23800 sequences...\n",
            "Serialized 23900 sequences...\n",
            "Serialized 24000 sequences...\n",
            "Serialized 24100 sequences...\n",
            "Serialized 24200 sequences...\n",
            "Serialized 24300 sequences...\n",
            "Serialized 24400 sequences...\n",
            "Serialized 24500 sequences...\n",
            "Serialized 24600 sequences...\n",
            "Serialized 24700 sequences...\n",
            "Serialized 24800 sequences...\n",
            "Serialized 24900 sequences...\n",
            "Serialized 25000 sequences...\n",
            "Serialized 25100 sequences...\n",
            "Serialized 25200 sequences...\n",
            "Serialized 25300 sequences...\n",
            "Serialized 25400 sequences...\n",
            "Serialized 25500 sequences...\n",
            "Serialized 25600 sequences...\n",
            "Serialized 25700 sequences...\n",
            "Serialized 25800 sequences...\n",
            "Serialized 25900 sequences...\n",
            "Serialized 26000 sequences...\n",
            "Serialized 26100 sequences...\n",
            "Serialized 26200 sequences...\n",
            "Serialized 26300 sequences...\n",
            "Serialized 26400 sequences...\n",
            "Serialized 26500 sequences...\n",
            "Serialized 26600 sequences...\n",
            "Serialized 26700 sequences...\n",
            "Serialized 26800 sequences...\n",
            "Serialized 26900 sequences...\n",
            "Serialized 27000 sequences...\n",
            "Serialized 27100 sequences...\n",
            "Serialized 27200 sequences...\n",
            "Serialized 27300 sequences...\n",
            "Serialized 27400 sequences...\n",
            "Serialized 27500 sequences...\n",
            "Serialized 27600 sequences...\n",
            "Serialized 27700 sequences...\n",
            "Serialized 27800 sequences...\n",
            "Serialized 27900 sequences...\n",
            "Serialized 28000 sequences...\n",
            "Serialized 28100 sequences...\n",
            "Serialized 28200 sequences...\n",
            "Serialized 28300 sequences...\n",
            "Serialized 28400 sequences...\n",
            "Serialized 28500 sequences...\n",
            "Serialized 28600 sequences...\n",
            "Serialized 28700 sequences...\n",
            "Serialized 28800 sequences...\n",
            "Serialized 28900 sequences...\n",
            "Serialized 29000 sequences...\n",
            "Serialized 29100 sequences...\n",
            "Serialized 29200 sequences...\n",
            "Serialized 29300 sequences...\n",
            "Serialized 29400 sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y_Lz0_6FVfh",
        "colab_type": "code",
        "outputId": "30a3ee04-5337-438b-8388-81f363f60985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from prepare_data2 import parse_seq\n",
        "import pickle\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"shake.tfrecords\")\n",
        "\n",
        "data = data.map(lambda x: parse_seq(x))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"shake_vocab\", mode=\"rb\")) \n",
        "#unpickling-convert byte stream to python object structure #read bytes\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab_size) \n",
        "print(vocab) \n",
        "print(ind_to_ch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70\n",
            "{'E': 3, '3': 4, ',': 5, 't': 6, 'N': 7, 'P': 8, 'X': 9, 'w': 10, 'h': 11, 'o': 12, 'L': 13, 'g': 14, 'f': 15, 'p': 16, 'H': 17, '-': 18, 'Z': 19, 'k': 20, 'U': 21, 'O': 22, 'W': 23, 'M': 24, ';': 25, 'V': 26, 's': 27, 'c': 28, 'T': 29, 'S': 30, 'j': 31, 'q': 32, '[': 33, 'K': 34, 'F': 35, ':': 36, ']': 37, 'R': 38, 'J': 39, 'm': 40, 'b': 41, 'x': 42, '?': 43, '!': 44, \"'\": 45, 'Y': 46, '$': 47, 'B': 48, 'v': 49, 'C': 50, '\\n': 51, '.': 52, 'd': 53, '&': 54, 'i': 55, 'z': 56, 'G': 57, 'I': 58, 'e': 59, 'r': 60, 'a': 61, 'n': 62, 'y': 63, 'u': 64, 'A': 65, 'D': 66, ' ': 67, 'l': 68, 'Q': 69, '<PAD>': 0, '<S>': 1, '</S>': 2}\n",
            "{3: 'E', 4: '3', 5: ',', 6: 't', 7: 'N', 8: 'P', 9: 'X', 10: 'w', 11: 'h', 12: 'o', 13: 'L', 14: 'g', 15: 'f', 16: 'p', 17: 'H', 18: '-', 19: 'Z', 20: 'k', 21: 'U', 22: 'O', 23: 'W', 24: 'M', 25: ';', 26: 'V', 27: 's', 28: 'c', 29: 'T', 30: 'S', 31: 'j', 32: 'q', 33: '[', 34: 'K', 35: 'F', 36: ':', 37: ']', 38: 'R', 39: 'J', 40: 'm', 41: 'b', 42: 'x', 43: '?', 44: '!', 45: \"'\", 46: 'Y', 47: '$', 48: 'B', 49: 'v', 50: 'C', 51: '\\n', 52: '.', 53: 'd', 54: '&', 55: 'i', 56: 'z', 57: 'G', 58: 'I', 59: 'e', 60: 'r', 61: 'a', 62: 'n', 63: 'y', 64: 'u', 65: 'A', 66: 'D', 67: ' ', 68: 'l', 69: 'Q', 0: '<PAD>', 1: '<S>', 2: '</S>'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8imBg9eq2-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = data.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnE4J4MPr76o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "dataset= dataset.shuffle(46000).repeat()\n",
        "dataset=dataset.padded_batch(BATCH_SIZE, padded_shapes=([499],[499]),drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zlW3Q1OslYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQtIiPDQsmHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, 499]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q71lXdHGsYW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoJChgOws3cR",
        "colab_type": "code",
        "outputId": "936c0d00-7f91-4c9b-d961-d3fb407a7420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, 499, 256)            17920     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (64, 499, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, 499, 70)             71750     \n",
            "=================================================================\n",
            "Total params: 4,027,974\n",
            "Trainable params: 4,027,974\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxMuH5IsTIsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqFuS3Fg1Zll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=predictions, name=None)\n",
        "    zero_count = tf.math.count_nonzero(inp, axis=1, keepdims=None, dtype=tf.dtypes.int64, name=None) - 1\n",
        "    mask = tf.sequence_mask(zero_count, 499,dtype=tf.dtypes.float32)\n",
        "    masked_loss = tf.math.multiply(loss, mask)\n",
        "    red_masked_loss = tf.cast(tf.math.reduce_sum(masked_loss, axis=[0, 1], keepdims=False, name=None), tf.float32)\n",
        "    total_zero_count = tf.cast(tf.reduce_sum(zero_count, 0), tf.float32)\n",
        "    final_loss = red_masked_loss / total_zero_count\n",
        "    \n",
        "  grads = tape.gradient(final_loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return final_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ec_40LoTTN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51UGdcaJTVqm",
        "colab_type": "code",
        "outputId": "9de374ee-5b4f-4428-a5fa-159514396e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    if batch_n > 300:\n",
        "      break\n",
        "\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.249100208282471\n",
            "Epoch 1 Batch 100 Loss 2.3698971271514893\n",
            "Epoch 1 Batch 200 Loss 2.131608486175537\n",
            "Epoch 1 Batch 300 Loss 1.8823298215866089\n",
            "Epoch 1 Loss 1.8823\n",
            "Time taken for 1 epoch 192.9318413734436 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.9282201528549194\n",
            "Epoch 2 Batch 100 Loss 1.7172625064849854\n",
            "Epoch 2 Batch 200 Loss 1.6215095520019531\n",
            "Epoch 2 Batch 300 Loss 1.5679713487625122\n",
            "Epoch 2 Loss 1.5680\n",
            "Time taken for 1 epoch 192.51146411895752 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.5721690654754639\n",
            "Epoch 3 Batch 100 Loss 1.5134748220443726\n",
            "Epoch 3 Batch 200 Loss 1.429953932762146\n",
            "Epoch 3 Batch 300 Loss 1.4265996217727661\n",
            "Epoch 3 Loss 1.4266\n",
            "Time taken for 1 epoch 194.7809624671936 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.4064254760742188\n",
            "Epoch 4 Batch 100 Loss 1.4210389852523804\n",
            "Epoch 4 Batch 200 Loss 1.3520255088806152\n",
            "Epoch 4 Batch 300 Loss 1.3290760517120361\n",
            "Epoch 4 Loss 1.3291\n",
            "Time taken for 1 epoch 194.2234833240509 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.3120431900024414\n",
            "Epoch 5 Batch 100 Loss 1.3017184734344482\n",
            "Epoch 5 Batch 200 Loss 1.2747375965118408\n",
            "Epoch 5 Batch 300 Loss 1.2415661811828613\n",
            "Epoch 5 Loss 1.2416\n",
            "Time taken for 1 epoch 194.20787501335144 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3333384990692139\n",
            "Epoch 6 Batch 100 Loss 1.2958292961120605\n",
            "Epoch 6 Batch 200 Loss 1.266701340675354\n",
            "Epoch 6 Batch 300 Loss 1.237235188484192\n",
            "Epoch 6 Loss 1.2372\n",
            "Time taken for 1 epoch 193.58733487129211 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2046903371810913\n",
            "Epoch 7 Batch 100 Loss 1.2188875675201416\n",
            "Epoch 7 Batch 200 Loss 1.2023855447769165\n",
            "Epoch 7 Batch 300 Loss 1.253812313079834\n",
            "Epoch 7 Loss 1.2538\n",
            "Time taken for 1 epoch 193.62983918190002 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.206009030342102\n",
            "Epoch 8 Batch 100 Loss 1.2035627365112305\n",
            "Epoch 8 Batch 200 Loss 1.222429871559143\n",
            "Epoch 8 Batch 300 Loss 1.1987226009368896\n",
            "Epoch 8 Loss 1.1987\n",
            "Time taken for 1 epoch 193.27178645133972 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.1523871421813965\n",
            "Epoch 9 Batch 100 Loss 1.1361660957336426\n",
            "Epoch 9 Batch 200 Loss 1.2014762163162231\n",
            "Epoch 9 Batch 300 Loss 1.1795367002487183\n",
            "Epoch 9 Loss 1.1795\n",
            "Time taken for 1 epoch 192.19747972488403 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.1502625942230225\n",
            "Epoch 10 Batch 100 Loss 1.1404727697372437\n",
            "Epoch 10 Batch 200 Loss 1.1135178804397583\n",
            "Epoch 10 Batch 300 Loss 1.1489574909210205\n",
            "Epoch 10 Loss 1.1490\n",
            "Time taken for 1 epoch 193.4224305152893 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0DA3-7Ltwyy",
        "colab_type": "code",
        "outputId": "186f5f63-53ac-4aa9-f27d-51801bcc53ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_9'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LICmAVvgt52X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "#suppress warning messages\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kup96nV_t9zz",
        "colab_type": "code",
        "outputId": "9fe8475a-432c-4f30-dd16-4fa17d1e2540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (1, 499, 256)             17920     \n",
            "_________________________________________________________________\n",
            "gru_12 (GRU)                 (1, 499, 1024)            3938304   \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (1, 499, 70)              71750     \n",
            "=================================================================\n",
            "Total params: 4,027,974\n",
            "Trainable params: 4,027,974\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVLpHSdos7Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [vocab[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(ind_to_ch[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRGDbaCItS6p",
        "colab_type": "code",
        "outputId": "b8b2de4f-f82c-4a75-be2a-1f0820044e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"KING: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KING: Cheris tame their fruit;\n",
            "And woman shall I eat the senate secrech\n",
            "Could hold over from us and you take hole.\n",
            "May I say my heart? bring forth now will say some\n",
            "than thou dost heavy to you?\n",
            "What, thou couldst not? I toick'd but hither,\n",
            "Do in all distemption to Speak or in,\n",
            "Where you shall be so 'twas wrought here to leave.\n",
            "Honderful at the meral, that hope stands straight.\n",
            "Come to perfect, I am a-mad: an if more\n",
            "might not till then, when you will not have merry.\n",
            "What state, the queen's fashion, I say! the bush of them\n",
            "Shall never seen it from the better, and the witness\n",
            "Which you forgotten to this medlia in your\n",
            "stripsting, as I wak it, I do triumph to-night,\n",
            "Because it were down: I am command thee! is it\n",
            "do assist. I progument ro sprishet the penny\n",
            "of much lodies: 'tis so well deserved:\n",
            "I were pity following:\n",
            "as he promised buy their majestion!\n",
            "First, for thee in the way of better weems;\n",
            "A moral eye, in England and my foot,\n",
            "That is retentivold as her willer and dull, lo,\n",
            "Wherein commend\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}